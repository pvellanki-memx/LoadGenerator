= FRS: Surveillance Engine Functional Requirements
:sectnums:
:toc:

== Overview

The Surveillance Engine is a critical component of the Trading Surveillance System. Its primary function is to analyze trade and order data in real-time to detect and alert on potential market abuses and irregularities in trading activities. The functional requirements outlined here align with the objectives and scope detailed in the Surveillance BRD.

== Functional Requirements

=== FR1: Data Ingestion and Normalization
- FR1.1: The engine must ingest real-time trade and order data continuously with no data loss.
- FR1.2: Data normalization must conform to a predefined schema to ensure consistency across all data points.

=== FR2: Benchmarking Integration
- FR2.1: The engine shall integrate with the Benchmarking module to retrieve current benchmark parameters for various trade instruments.
- FR2.2: It should automatically adjust the detection algorithms based on updated benchmark data.

=== FR3: Pattern Recognition and Anomaly Detection
- FR3.1: Must identify known patterns of market manipulation, such as spoofing, wash trades, and layering, based on configurable rules and benchmarks.
- FR3.2: Should use statistical analysis to detect anomalies that could indicate market manipulation or irregularities.

=== FR4: Alert Generation
- FR4.1: The engine is required to generate alerts for any detected activities that deviate from the benchmark or match known patterns of market abuse.
- FR4.2: Each alert must include relevant details such as security identifier, timestamp, and a description of the detected irregularity.

=== FR5: Real-Time Processing
- FR5.1: Real-time data processing should occur with minimal latency to enable timely detection and alerting.
- FR5.2: Must support processing and analysis of data across multiple trading venues simultaneously.

=== FR6: Scalability
- FR6.1: The engine must be scalable to handle increasing volumes of data without degradation in performance.
- FR6.2: Should provide mechanisms to scale horizontally, adding more processing power as the data volume increases.

=== FR7: Historical Data Analysis
- FR7.1: Should have the capability to analyze historical data for back-testing and refining detection algorithms.
- FR7.2: Must maintain a historical database of alerts and outcomes to improve future detection accuracy.

=== FR8: User-Defined Rules and Parameters
- FR8.1: Must allow regulatory users to configure and modify the rules and parameters used for market abuse detection.
- FR8.2: Should include a user-friendly interface for setting these parameters without needing to change the underlying code.

=== FR9: Reporting and Audit Trails
- FR9.1: The engine must record all generated alerts along with decisions and actions taken as a result.
- FR9.2: Must provide comprehensive reporting capabilities for auditing and compliance purposes.

=== FR10: Fault Tolerance and Reliability
- FR10.1: Must have failover capabilities to ensure continuous operation.
- FR10.2: Should maintain data integrity and provide accurate processing outcomes even in the event of partial system failures.

== Validation Criteria

Each functional requirement will have associated validation tests to ensure they are met. The system will be subjected to various scenarios, including simulated market conditions, to validate the accuracy and efficiency of the Surveillance Engine.

== Compliance and Performance Standards

The Surveillance Engine must comply with relevant financial industry regulations and standards. It must also meet defined performance benchmarks, including but not limited to processing latency and alert accuracy rates.




= SPEC-02: Trading Surveillance System with Benchmarking
:sectnums:
:toc:

== Background

The enhanced Trading Surveillance System is an advanced solution designed for real-time monitoring of trading activities, with an emphasis on data-driven benchmarking to identify anomalies and potential manipulative behaviors. This system integrates a sophisticated benchmarking process to calibrate the surveillance algorithms, leveraging InfluxDB for time-series data storage, enabling efficient and rapid detection of unusual patterns.

== Requirements

=== Functional Requirements
- Benchmark-based alerting for identifying deviations from established trading patterns.
- Real-time analysis of trades against historical data in InfluxDB.
- Intuitive User Interface for surveillance monitoring and alert management.
- Efficient workflow for alert investigation and case management.

=== Non-Functional Requirements
- Benchmarking process should be automated and run at configurable intervals.
- The Surveillance Engine must process data with minimal latency.
- The User Interface should provide real-time updates without perceptible delay.
- Alert Management & Workflow system must support concurrent use by multiple operators.

== Method

=== Architecture Overview
The system is composed of distinct but interconnected components:
- **Benchmarking**: Sets the baseline for normal trading activities.
- **InfluxDB**: Stores time-series data used for benchmarking and real-time analysis.
- **Surveillance Engine**: Analyzes streaming data, compares against benchmarks, and generates alerts.
- **User Interface**: Allows users to view and manage alerts.
- **Alert Management & Workflow**: Manages the lifecycle of alerts and supports case investigation processes.

=== Component Design

==== Benchmarking
- Determines thresholds and patterns that represent normal trading behavior.
- Periodically updates to adapt to changing market conditions.
- Supplies the Surveillance Engine with dynamic parameters for anomaly detection.

==== InfluxDB
- Stores granular trade and order data with precise timestamping.
- Provides fast querying capabilities for real-time analytics.
- Serves as the historical data repository for the benchmarking process.

==== Surveillance Engine
- Processes data streams in real-time, leveraging benchmarks for anomaly detection.
- Generates alerts for activities that significantly deviate from the benchmark.
- Integrates with InfluxDB for continuous data retrieval and analysis.

==== User Interface
- Displays a dashboard for real-time surveillance updates.
- Provides tools for configuring benchmark parameters and alert thresholds.
- Offers investigative functionalities for deep dives into specific alerts.

==== Alert Management & Workflow
- Tracks the status of each alert from generation to resolution.
- Supports assignment, acknowledgment, and documentation of investigative actions.
- Integrates audit trails for compliance and retrospective analysis.

== Implementation

The implementation process follows the WBS structure with specified tasks and timelines.

=== System Setup and Configuration
Involves configuring InfluxDB, setting up the benchmarking module, and establishing the initial parameters for the Surveillance Engine.

=== Development
Encompasses the development of the Surveillance Engine with benchmarking integration, the User Interface design and creation, and the Alert Management & Workflow system.

=== Integration and Testing
Focuses on integrating all components, ensuring data flows correctly, and validating the functionality of the entire system.

=== Deployment
Includes staging and production deployment, ensuring the system is operational and ready for use.

== Milestones
- M1: Completion of InfluxDB Setup and Benchmarking Integration
- M2: Development of Surveillance Engine and UI Completion
- M3: Full System Integration
- M4: Successful Staging Deployment
- M5: Production Deployment and System Go-Live
- M6: Post-Deployment Review and Operational Handoff

== Gathering Results
Evaluates system performance based on the accuracy and efficiency of alert generation, benchmarking effectiveness, and user feedback on the Interface and Workflow system.













import pandas as pd
import sqlite3
import sys
import os

db_name = sys.argv[1]
# Connect to the SQLite database
conn = sqlite3.connect(db_name)

# Query the data from the grouped_trades table
query = "SELECT * FROM grouped_trades"
grouped_trade_data_frame = pd.read_sql_query(query, conn)

# Close the database connection
conn.close()

# Symbols to handle separately
symbols = ['SPY', 'QQQ', 'TSLA', 'AAPL', 'NVDA', 'IWM']

# Base file name from the database name
base_file_name = db_name[5:10]

# Loop through each symbol and save the corresponding CSV
for symbol in symbols:
    df_filtered = grouped_trade_data_frame[grouped_trade_data_frame['SYM'] == symbol]
    output_file_name = f'summary_trade_data_{base_file_name}_{symbol}.csv'
    df_filtered.to_csv(output_file_name, index=False)

# Handle all other symbols
other_symbols_df = grouped_trade_data_frame[~grouped_trade_data_frame['SYM'].isin(symbols)]
output_file_name = f'summary_trade_data_{base_file_name}_OTHERS.csv'
other_symbols_df.to_csv(output_file_name, index=False)

print("Data from grouped_trades has been successfully processed and saved.")



http://ec2-35-170-218-11.compute-1.amazonaws.com/



tail -n +2 file.csv | wc -l

 sftp -i /home/pvellanki/memx.pem memx@ec2-35-170-218-11.compute-1.amazonaws.com <<< $'put OptionsQuoteRequestComponentsState_1B.csv.gz'


sftp -i /path/to/your/private_key.pem username@ec2-35-170-218-11.compute-1.amazonaws.com:/path/on/server/ <<< $'put /path/to/local/file'


du -h --max-depth=1 /path/to/directory | sort -hr

chmod 400 /home/pvellanki/memx.pem
ssh -i /home/pvellanki/memx.pem memx@ec2-35-170-218-11.compute-1.amazonaws.com

[root@sys0412 ~]# sftp -v -i /home/pvellanki/memx.pem memx@ec2-35-170-218-11.compute-1.amazonaws.com <<< $'put OptionsQuoteRequestComponentsState_1B.csv.gz'
OpenSSH_8.0p1, OpenSSL 1.1.1g FIPS  21 Apr 2020
debug1: Reading configuration data /root/.ssh/config
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: Reading configuration data /etc/ssh/ssh_config.d/05-redhat.conf
debug1: Reading configuration data /etc/crypto-policies/back-ends/openssh.config
debug1: configuration requests final Match pass
debug1: re-parsing configuration
debug1: Reading configuration data /root/.ssh/config
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: Reading configuration data /etc/ssh/ssh_config.d/05-redhat.conf
debug1: Reading configuration data /etc/crypto-policies/back-ends/openssh.config
debug1: Connecting to ec2-35-170-218-11.compute-1.amazonaws.com [35.170.218.11] port 22.
debug1: Connection established.
debug1: identity file /home/pvellanki/memx.pem type -1
debug1: identity file /home/pvellanki/memx.pem-cert type -1
debug1: Local version string SSH-2.0-OpenSSH_8.0
debug1: Remote protocol version 2.0, remote software version OpenSSH_7.6p1 Ubuntu-4ubuntu0.5
debug1: match: OpenSSH_7.6p1 Ubuntu-4ubuntu0.5 pat OpenSSH_7.0*,OpenSSH_7.1*,OpenSSH_7.2*,OpenSSH_7.3*,OpenSSH_7.4*,OpenSSH_7.5*,OpenSSH_7.6*,OpenSSH_7.7* compat 0x04000002
debug1: Authenticating to ec2-35-170-218-11.compute-1.amazonaws.com:22 as 'memx'
debug1: SSH2_MSG_KEXINIT sent
debug1: SSH2_MSG_KEXINIT received
debug1: kex: algorithm: curve25519-sha256
debug1: kex: host key algorithm: ecdsa-sha2-nistp256
debug1: kex: server->client cipher: aes256-gcm@openssh.com MAC: <implicit> compression: none
debug1: kex: client->server cipher: aes256-gcm@openssh.com MAC: <implicit> compression: none
debug1: kex: curve25519-sha256 need=32 dh_need=32
debug1: kex: curve25519-sha256 need=32 dh_need=32
debug1: expecting SSH2_MSG_KEX_ECDH_REPLY
debug1: Server host key: ecdsa-sha2-nistp256 SHA256:8d35+0MZ+YFpAGHMV+6xcfCDJpWgkOUUdXIGXYdXwc8
debug1: Host 'ec2-35-170-218-11.compute-1.amazonaws.com' is known and matches the ECDSA host key.
debug1: Found key in /root/.ssh/known_hosts:4
debug1: rekey out after 4294967296 blocks
debug1: SSH2_MSG_NEWKEYS sent
debug1: expecting SSH2_MSG_NEWKEYS
debug1: SSH2_MSG_NEWKEYS received
debug1: rekey in after 4294967296 blocks
debug1: Will attempt key: /home/pvellanki/memx.pem  explicit
debug1: SSH2_MSG_EXT_INFO received
debug1: kex_input_ext_info: server-sig-algs=<ssh-ed25519,ssh-rsa,rsa-sha2-256,rsa-sha2-512,ssh-dss,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521>
debug1: SSH2_MSG_SERVICE_ACCEPT received
debug1: Authentications that can continue: publickey
debug1: Next authentication method: publickey
debug1: Trying private key: /home/pvellanki/memx.pem
debug1: Authentication succeeded (publickey).
Authenticated to ec2-35-170-218-11.compute-1.amazonaws.com ([35.170.218.11]:22).
debug1: channel 0: new [client-session]
debug1: Requesting no-more-sessions@openssh.com
debug1: Entering interactive session.
debug1: pledge: network
debug1: client_input_global_request: rtype hostkeys-00@openssh.com want_reply 0
debug1: Sending environment.
debug1: Sending env LANG = en_US.UTF-8
debug1: Sending subsystem: sftp
debug1: client_input_channel_req: channel 0 rtype exit-status reply 0
debug1: client_input_channel_req: channel 0 rtype eow@openssh.com reply 0
debug1: channel 0: free: client-session, nchannels 1
debug1: fd 0 clearing O_NONBLOCK
Transferred: sent 2704, received 2288 bytes, in 0.6 seconds
Bytes per second: sent 4334.1, received 3667.4
debug1: Exit status 1
Connection closed


mv *"${mm_dd}"*.csv "$output_dir/"



import pandas as pd
import sqlite3
import sys
import os
import numpy as np

db_name = sys.argv[1]
# Connect to the SQLite database
conn = sqlite3.connect(db_name)

# Query the data from the grouped_trades table
query = "SELECT * FROM grouped_trades"
grouped_trade_data_frame = pd.read_sql_query(query, conn)

# Close the database connection
conn.close()

# Symbols to handle separately
symbols = ['SPY', 'QQQ', 'TSLA', 'AAPL', 'NVDA', 'IWM']

# Base file name from the database name
base_file_name = db_name[5:10]

# Loop through each symbol and save the corresponding CSV
for symbol in symbols:
    df_filtered = grouped_trade_data_frame[grouped_trade_data_frame['SYM'] == symbol]
    output_file_name = f'summary_trade_data_{base_file_name}_{symbol}.csv'
    df_filtered.to_csv(output_file_name, index=False)

# Handle all other symbols and split into two parts
other_symbols_df = grouped_trade_data_frame[~grouped_trade_data_frame['SYM'].isin(symbols)]

# Splitting the DataFrame into two parts
split_dfs = np.array_split(other_symbols_df, 2)

# Save each part to a separate file
for i, df_part in enumerate(split_dfs, start=1):
    output_file_name = f'summary_trade_data_{base_file_name}_OTHERS_part{i}.csv'
    df_part.to_csv(output_file_name, index=False)

print("Data from grouped_trades has been successfully processed and saved.")

